{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce59607d",
   "metadata": {},
   "source": [
    "# Critical Analysis :Comp2200 46282858 Sangeun Lee 2/11/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920e47c",
   "metadata": {},
   "source": [
    "##  1. Handling null values\n",
    " If null values occur in the data, problems may arise when modelling or predicting the result. Therefore, when loading a data set, you must check whether there are null values in the data set and not keep them in the data set. There are two main methods for dealing with null values. The first is to delete rows containing null values. However, if the dataset has more than 10% null values of the entire dataset, a different solution needs to be considered. This is because reducing the size of the data can decrease the accuracy of the predicted values. What other method do we use is replace a null value with another value? The possible values are the average or median of each variable.  We need to avoid inserting any values because inserting any values may affect the overall dataset. Take a look at the provided notebook file. There is an error in data cleaning of critical analysis. The variable of not.fully.paid contains one missing value. In the provided file, it has been replaced with the mean value of that. Below is a picture of the top 5 rows showing the head function of the dataset replaced with the mean value for the top 5 rows.\n",
    " <img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix1.png\" width=\"700\" height=\"500\"/>\n",
    "The variable is not.fully.paid, which is a nominal variable. Within imputing these variables, KNN can be used or filled with 0 or mode values. This variable represents Whether the borrower will be fully paid or not as binary 0 and 1. However, the value of the top row is 0.160071, which is not expected. Therefore, if data mining is performed with this value, the accuracy of the model may decrease. Again, there is only one error in the entire data, the current model might not be significantly affected. However, if such variables account for more than 10% of the dataset, errors can occur. A solution would be to perform the model without the rows with missing values. Likewise, given data contains a very small number of missing values, data will have very minimal impact on modelling. The following is the code modified for accurate data cleaning.\n",
    "```\n",
    "Data = Data.dropna(axis=0)\n",
    "Data.info()\n",
    "Data.head()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix2.png\" width=\"700\" height=\"500\"/>\n",
    "The picture above shows the data set after deleting the data. The number of rows with missing values was less than 10% of the total, decided to remove that. Consequently, the total number of rows was reduced to 9577. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005329d",
   "metadata": {},
   "source": [
    "## 2. Correlation \n",
    "상관관계의 정도를 고려할 때 우리는 그 correlation값을 절대값으로 고려해야 한다. negative correlation 은 하나의 변수가 증가할 때 상대 변수의 값은 감소함을 뜻한다. positive relation 과 반대의 영향을 서로 끼친다고 해석한다. 예를들어 credit.policy 와 int.rate 사이의 correlatio n은 -0.29이다. If interest rate of the loan is higher, in case of the rate of customer meets the credit underwriting criteria  is decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da24cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
