{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98097809",
   "metadata": {},
   "source": [
    "# Critical Analysis :Comp2200 46282858 Sangeun Lee 2/11/23\n",
    "---\n",
    "## Introduction\n",
    "This report identifies the problems with the provided analysis and suggests solutions to make the model perform more successfully. There are three issues discovered, and some sample codes and images are provided to solve them and develop the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b1561",
   "metadata": {},
   "source": [
    "##  1. Handling null values\n",
    " If null values occur in the data, problems may arise when modelling or predicting the result. Therefore, when loading a data set, you must check whether there are null values in the data set and not keep them in the data set. There are two main methods for dealing with null values. The first is to delete rows containing null values. However, if the dataset has more than 10% null values of the entire dataset, a different solution needs to be considered. Because reducing the size of the data can decrease the accuracy of the predicted values. What other method do we use to replace a null value with another value? The possible values are the average or median of each variable. We need to avoid inserting any values because inserting any values may affect the overall dataset negatively. Take a look at the provided notebook file. There is an error in data cleaning of critical analysis. The variable of not.fully.paid contains one missing value. In the provided file, it has been replaced with the mean value. Below is a picture of the top 5 rows showing the variable replaced with the mean value of not.fully.paid. \n",
    " <img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix1.png\" width=\"700\" height=\"500\"/>\n",
    "The variable is not.fully.paid is a nominal variable. Within imputing these variables, KNN can be used or filled with 0. This variable represents Whether the borrower will be fully paid or not as binary 0 and 1. However, the value of the top row is 0.160071, which is not expected. Therefore, if data mining is performed with this value, the accuracy of the model may decrease. Again, there is only one error in the entire data, the current model might not be significantly affected. However, if such variables account for more than 10% of the dataset, errors can occur. A solution would be to perform the model without the rows with missing values. The following is the code modified for accurate data cleaning.\n",
    "```\n",
    "Data = Data.dropna(axis=0)\n",
    "Data.info()\n",
    "Data.head()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix2.png\" width=\"700\" height=\"500\"/>\n",
    "The picture above shows the data set after deleting the data. The number of rows with missing values was less than 10% of the total, decided to remove that. Consequently, the total number of rows was reduced to 9577. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d01590",
   "metadata": {},
   "source": [
    "## 2. Correlation \n",
    "When considering the size of the correlation, we must consider the correlation value as an absolute value. The negative correlation means that when one variable increases, the other variable tends to decrease. A positive correlation means that both variables affect each other positively. For example, the correlation between credit.policy and int.rate is -0.29. This means that if the interest rate of the loan is higher, in case the rate if a customer meets the credit underwriting criteria decreases by 29%. Therefore, whether the correlation has a positive or negative is not a factor to consider when including variables in the model. This is the guidance of the standard of correlation. Generally, if the absolute value of the correlation coefficient is above 0.9, the relationship between the two variables is considered very high.  Between 0.7 to 0.5 is high.  Below 0.4 can be evaluated as low. Typically, a relationship must exist between the independent variable and the dependent variable to be reflected in the model analysis. The following is the correlation heatmap. Since there are so many variables, it is difficult to understand them all at once, so I summarized only the bottom for clarity.\n",
    " \n",
    "```\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "ax1_half = np.triu(np.ones(Data.corr('spearman').shape), k=0)  \n",
    "sns.heatmap(Data.corr('spearman'), annot=True, fmt=\".2f\", cmap=\"RdYlBu\", ax=ax1, mask=mask)\n",
    "ax1.set_title(\"Correlation Heat Map - Spearman Coeff\", fontsize=14)\n",
    "\n",
    "ax2_half = np.triu(np.ones(Data.corr('pearson').shape), k=0)  \n",
    "sns.heatmap(Data.corr('pearson'), annot=True, fmt=\".2f\", cmap=\"RdYlBu\", ax=ax2, mask=mask)\n",
    "ax2.set_title(\"Correlation Heat Map - Pearson Coeff\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix3.png\" width=\"700\" height=\"500\"/>\n",
    "Most of the correlations with the dependent variable are very low. The variable with the highest correlation with the dependent variable is inq.last.6mths, showing the highest value in both methods. The variables that have a negative relation with the dependence variable are int.rate, dti, revol.bal, revol.util, inq.last.6mths,delinq.2yrs, pub.rec, not.fully.paid. Otherwise, the variables installment, log.anual.inc, fico, days.with.cr.line have a positive correlation with the dependence variable. As mentioned in the provided analysis, having a negative correlation does not necessarily \bguarantee excluding it from the model. However, if the correlation between independent variables is high, one might consider whether to include these variables in the model. High correlations between independent variables that one independent variable significantly influences one another, thus it might be considered for removal from the model. Typically, an absolute value of the correlation between independent variables over 0.9 is excluded from the model. However, with a correlation over 0.7, it may have an impact on the model so needs to be observed. The decisions that which variables included in the model should be made depending on the estimates from the model and the correlations. Besides, variables also can be removed depending on the builder's interpretation. In the provided data, there is a high correlation of more than 0.7 between int.rate and fico, although not more than 0.9. It can be evaluated that this is unlikely to lead to a multicollinearity problem.  However, it is necessary to verify the estimates to ensure that they do not impact each other. Moreover, the variables should be removed as long as generate a lot of noise by performing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b8b47",
   "metadata": {},
   "source": [
    "## 3. Normalization for data set\n",
    "Before splitting the data, normalizing it results in an ideally standardized dataset with a mean of 0 and a variance of 1. However, this can lead to overly optimistic and unrealistic performance evaluations.  In the provided Jupyter notebook, the data was split before it scaled. \n",
    "Nevertheless, there is an error. The following codes are normalization performed from the given analysis.\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "obje_ss=StandardScaler()\n",
    "x_ex1_train=obje_ss.fit_transform(x_ex1_train)\n",
    "x_ex1_test=obje_ss.fit_transform(x_ex1_test)\n",
    "```\n",
    "As mentioned above, to prevent overly biased results, it is necessary to scale the data after splitting it. In the executed code, normalization was performed after splitting the data, affecting the test set as well. Also, the y variable in the train data was not normalized. If using scaled data as above, can lead to overly optimistic prediction results.   The following is the code that has been updated to complement these concerns. \n",
    "```\n",
    "x_ex1_train=obje_ss.fit_transform(x_ex1_train)\n",
    "y_ex1_train=obje_ss.fit_transform(y_ex1_train)\n",
    "```\n",
    "With the code, a training data set with a mean of 0 and a variance of 1 was completed. Scaling was not applied to the test set, and normalization was applied only to the train set to prevent unrealistic predictions of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f49c70",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "It may be necessary to design a model based on the knowledge and expertise of the model's creator rather than relying on numerical values for model execution. For a successful model design, it is essential to thoroughly understand the correlations between variables and comprehend the data before model execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec705bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
