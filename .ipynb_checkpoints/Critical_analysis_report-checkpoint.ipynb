{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a76cb54",
   "metadata": {},
   "source": [
    "# Critical Analysis :Comp2200 46282858 Sangeun Lee 2/11/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22490dcc",
   "metadata": {},
   "source": [
    "##  1. Handling null values\n",
    " If null values occur in the data, problems may arise when modelling or predicting the result. Therefore, when loading a data set, you must check whether there are null values in the data set and not keep them in the data set. There are two main methods for dealing with null values. The first is to delete rows containing null values. However, if the dataset has more than 10% null values of the entire dataset, a different solution needs to be considered. This is because reducing the size of the data can decrease the accuracy of the predicted values. What other method do we use is replace a null value with another value? The possible values are the average or median of each variable.  We need to avoid inserting any values because inserting any values may affect the overall dataset. Take a look at the provided notebook file. There is an error in data cleaning of critical analysis. The variable of not.fully.paid contains one missing value. In the provided file, it has been replaced with the mean value of that. Below is a picture of the top 5 rows showing the head function of the dataset replaced with the mean value for the top 5 rows.\n",
    " <img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix1.png\" width=\"700\" height=\"500\"/>\n",
    "The variable is not.fully.paid, which is a nominal variable. Within imputing these variables, KNN can be used or filled with 0 or mode values. This variable represents Whether the borrower will be fully paid or not as binary 0 and 1. However, the value of the top row is 0.160071, which is not expected. Therefore, if data mining is performed with this value, the accuracy of the model may decrease. Again, there is only one error in the entire data, the current model might not be significantly affected. However, if such variables account for more than 10% of the dataset, errors can occur. A solution would be to perform the model without the rows with missing values. Likewise, given data contains a very small number of missing values, data will have very minimal impact on modelling. The following is the code modified for accurate data cleaning.\n",
    "```\n",
    "Data = Data.dropna(axis=0)\n",
    "Data.info()\n",
    "Data.head()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix2.png\" width=\"700\" height=\"500\"/>\n",
    "The picture above shows the data set after deleting the data. The number of rows with missing values was less than 10% of the total, decided to remove that. Consequently, the total number of rows was reduced to 9577. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80807d8c",
   "metadata": {},
   "source": [
    "## 2. Correlation \n",
    "상관관계의 정도를 고려할 때 우리는 그 correlation값을 절대값으로 고려해야 한다. negative correlation 은 하나의 변수가 증가할 때 상대 변수의 값은 감소함을 뜻한다. positive relation 과 반대의 영향을 서로 끼친다고 해석한다. 예를들어 credit.policy 와 int.rate 사이의 correlatio n은 -0.29이다. If interest rate of the loan is higher, in case of the rate of customer meets the credit underwriting criteria  is decreases 29%. 따라서 correlation이 긍정적 영향을 끼치는지 부정적 영향을 끼치는 지는 모델에 변수를 포함시킬 때 고려되는 요소가 아니다. 보통 상관계수 절댓값이 0.9이상이라면 두 변수 사이의 상관관계가 아주 높다고 해석한다. 0.7에서 0.5는 높음. 0.4이하로는 낮다고 평가할 수 있다. 일반적으로 독립변수와 종속변수 사이에 관계가 존재해야 모델 분석에도 반영될 수 있다. 다음은 correlation heatmap 이다. 변수가 많아 한눈에 알아보기 어렵기 때문에 아래쪽만 남기고 정리해보았다.\n",
    "```\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "ax1_half = np.triu(np.ones(Data.corr('spearman').shape), k=0)  \n",
    "sns.heatmap(Data.corr('spearman'), annot=True, fmt=\".2f\", cmap=\"RdYlBu\", ax=ax1, mask=mask)\n",
    "ax1.set_title(\"Correlation Heat Map - Spearman Coeff\", fontsize=14)\n",
    "\n",
    "ax2_half = np.triu(np.ones(Data.corr('pearson').shape), k=0)  \n",
    "sns.heatmap(Data.corr('pearson'), annot=True, fmt=\".2f\", cmap=\"RdYlBu\", ax=ax2, mask=mask)\n",
    "ax2.set_title(\"Correlation Heat Map - Pearson Coeff\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix3.png\" width=\"700\" height=\"500\"/>\n",
    "종속변수와 가장 높은 상관관계를 갖는 변수는 inq.last.6mths로 두 방식의 correlation 모두에서 가장 높은 값을 갖는다. 대부분의 종속변수와의 correlation이 매우 낮다. int.rate와 fico사이에는 0.7이상의 높은 correlation을 갖는데 0.9이상은 아니기 때문에 이것은 다중공선성의 문제로 이어질 가능성은 적지만 추정치를 확인해볼 필요가 있다. 따라서 단순히 correlation 이 음의 상관관계를 띈다고 모델에서 제외할 수 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203997ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
