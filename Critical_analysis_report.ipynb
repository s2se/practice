{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd9bb0f",
   "metadata": {},
   "source": [
    "# Critical Analysis :Comp2200 46282858 Sangeun Lee 2/11/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e00b4",
   "metadata": {},
   "source": [
    "##  1. Handling null values\n",
    " If null values occur in the data, problems may arise when modelling or predicting the result. Therefore, when loading a data set, you must check whether there are null values in the data set and not keep them in the data set. There are two main methods for dealing with null values. The first is to delete rows containing null values. However, if the dataset has more than 10% null values of the entire dataset, a different solution needs to be considered. This is because reducing the size of the data can decrease the accuracy of the predicted values. What other method do we use is replace a null value with another value? The possible values are the average or median of each variable.  We need to avoid inserting any values because inserting any values may affect the overall dataset. Take a look at the provided notebook file. There is an error in data cleaning of critical analysis. The variable of not.fully.paid contains one missing value. In the provided file, it has been replaced with the mean value of that. Below is a picture of the top 5 rows showing the head function of the dataset replaced with the mean value for the top 5 rows.\n",
    " <img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix1.png\" width=\"700\" height=\"500\"/>\n",
    "The variable is not.fully.paid, which is a nominal variable. Within imputing these variables, KNN can be used or filled with 0 or mode values. This variable represents Whether the borrower will be fully paid or not as binary 0 and 1. However, the value of the top row is 0.160071, which is not expected. Therefore, if data mining is performed with this value, the accuracy of the model may decrease. Again, there is only one error in the entire data, the current model might not be significantly affected. However, if such variables account for more than 10% of the dataset, errors can occur. A solution would be to perform the model without the rows with missing values. Likewise, given data contains a very small number of missing values, data will have very minimal impact on modelling. The following is the code modified for accurate data cleaning.\n",
    "```\n",
    "Data = Data.dropna(axis=0)\n",
    "Data.info()\n",
    "Data.head()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix2.png\" width=\"700\" height=\"500\"/>\n",
    "The picture above shows the data set after deleting the data. The number of rows with missing values was less than 10% of the total, decided to remove that. Consequently, the total number of rows was reduced to 9577. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167686d0",
   "metadata": {},
   "source": [
    "## 2. Correlation \n",
    "상관관계의 정도를 고려할 때 우리는 그 correlation값을 절대값으로 고려해야 한다. negative correlation 은 하나의 변수가 증가할 때 상대 변수의 값은 감소함을 뜻한다. positive relation 과 반대의 영향을 서로 끼친다고 해석한다. 예를들어 credit.policy 와 int.rate 사이의 correlatio n은 -0.29이다. If interest rate of the loan is higher, in case of the rate of customer meets the credit underwriting criteria  is decreases 29%. 따라서 correlation이 긍정적 영향을 끼치는지 부정적 영향을 끼치는 지는 모델에 변수를 포함시킬 때 고려되는 요소가 아니다. 보통 상관계수 절댓값이 0.9이상이라면 두 변수 사이의 상관관계가 아주 높다고 해석한다. 0.7에서 0.5는 높음. 0.4이하로는 낮다고 평가할 수 있다. 일반적으로 독립변수와 종속변수 사이에 관계가 존재해야 모델 분석에도 반영될 수 있다. 다음은 correlation heatmap 이다. 변수가 많아 한눈에 알아보기 어렵기 때문에 아래쪽만 남기고 정리해보았다.\n",
    "```\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "ax1_half = np.triu(np.ones(Data.corr('spearman').shape), k=0)  \n",
    "sns.heatmap(Data.corr('spearman'), annot=True, fmt=\".2f\", cmap=\"RdYlBu\", ax=ax1, mask=mask)\n",
    "ax1.set_title(\"Correlation Heat Map - Spearman Coeff\", fontsize=14)\n",
    "\n",
    "ax2_half = np.triu(np.ones(Data.corr('pearson').shape), k=0)  \n",
    "sns.heatmap(Data.corr('pearson'), annot=True, fmt=\".2f\", cmap=\"RdYlBu\", ax=ax2, mask=mask)\n",
    "ax2.set_title(\"Correlation Heat Map - Pearson Coeff\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"https://raw.githubusercontent.com/s2se/s2se.github.io/main/image/appendix3.png\" width=\"700\" height=\"500\"/>\n",
    "종속변수와 가장 높은 상관관계를 갖는 변수는 inq.last.6mths로 두 방식의 correlation 모두에서 가장 높은 값을 갖는다. 대부분의 종속변수와의 correlation이 매우 낮다. 따라서 단순히 correlation 이 음의 상관관계를 띈다고 모델에서 제외할 수 없다. 다만 독립변수들 끼리의 correlation이 높다면 모델에 변수를 포함할지 고려할 수 있다. 독립변수들 사이 상관관계가 높으면 한 독립변수가 다른 독립변수에 큰 영향을 미친다는 것이기 때문에 이를 모델에서 제거할지 고려할 수 있다. 일반적으로 절대값 0.9이상의 값이라면 모델에서 제외하는 것으로 한다. 다만, 0.7이상의 상관관계를 갖더라도 모델에 영향을 미칠 수 있기 때문에 추정치를 확인하며 correlation에 따라 모델에 변수를 포함킬지 말지 결정해야 한다. 또한 설계자의 해석에 따라서도 변수를 제거할 수 있다. 제공된 데이터에서는 int.rate와 fico사이에서 0.7이상의 높은 correlation을 갖는데 0.9이상은 아니기 때문에 이것은 다중공선성의 문제로 이어질 가능성은 적다고 평가할 수 있다. 다만 추정치를 확인하여 서로 영향을 끼치지 않는지 확인할 필요가 있다. 또한 낮은 상관관계를 띄는 변수들을 중심으로 모델학습을 수행해 노이즈가 많이 발생하는 변수를 제거하도록 고려할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322c039",
   "metadata": {},
   "source": [
    "## 3. Normalization for data set\n",
    "데이터 splitting을 하기 전 데이터를 normalization 하면 전체 데이터가 평균 0, 분산 1로 이상적인 데이터 셋이 완성된다. This becomes can lead to over-optimistic results and unrealistic performance evaluations. 제공된 주피터 노트북에서는 데이터를 splitting 한 후 scaling 하였다. 하지만 여기에는 오류가 있다. 해당 코드는 제공된 anaylisis에서 수행된 Normalization 이다.\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "obje_ss=StandardScaler()\n",
    "x_ex1_train=obje_ss.fit_transform(x_ex1_train)\n",
    "x_ex1_test=obje_ss.fit_transform(x_ex1_test)\n",
    "```\n",
    "위에 언급한 것처럼 지나치게 이상적으로 편향되지 않은 결과를 얻기 위해서는 데이터 splitting 후 데이터를 scaling 해야 한다고 하였다. 하지만 위 제공된 코드에서는 테스트 셋 또한 normalization 되어있다. 위처럼 scaling 된 데이터를 사용한다면 지나치게 낙관적인 예측 결과가 발생할 수 있다. 수행된 코드에서는 splittig 후 데이터 normalization을 수행하였지만 test셋에도 적용하였다. 또한 train 데이터의 y변수는 정규화되지 않았다. 다음은 이를 보완하여 변경된 코드이다.\n",
    "```\n",
    "x_ex1_train=obje_ss.fit_transform(x_ex1_train)\n",
    "y_ex1_train=obje_ss.fit_transform(y_ex1_train)\n",
    "```\n",
    "변경된 코드로 평균 0, 분산 1인 트레이닝 데이터 셋이 완성되었다. test셋에는 scaling을 적용하지 않았고 오직 train set에만 정규화를 적용하여 결과가 비현실적으로 예측되는 것은 방지하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10e0c7",
   "metadata": {},
   "source": [
    "## 4. Unclear visualization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
